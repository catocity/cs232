# Comparing Text Generating Models

## Background
This project explores whether algorithms can be creative when learning, based on knowledge from music lyrics. The goal is to compare the performances of two language models: one that uses a Markov Chain and a second that uses a Recurrent Neural Network (RNN). For each model, song lyrics will be generated given a user’s input of genre, year, and/or artist. Performance will be evaluated using two scores. The first will be a comparison of similarities between the training data and the generated songs using a text comparison tool. The second will be a variation of the Turing test, in which human subjects will be shown lyrics and asked whether or not they think the lyrics were created by a human or a machine. 

## Motivation
We would like to better understand how machines generate text and how to recognize machine-generated text in real life. Furthermore, we are exploring bigger questions, like:
* What makes a song a song?
* What metrics can be used to evaluate the computer-generated lyrics?
* What does it means for something to be creative?
* Are computers demonstrating creativity through the computer-generated lyrics?

## Models and Tools

### Markov Chain

This model was implemented by Rob Dawson. Markov chains use probability to predict the next char/word in a sequence but they are unable to remember previous inputs in order to predict the following ones. 

https://github.com/codebox/markov-text

### Recurrent Neural Network
We decided to use a recurrent neural network created by Spiglerg to generate the lyrics. We chose this model because it is a simpler version of typical text generating neural networks, using Tensorflow. With the given model, we were able to adjust variables to control run time (and therefore, accuracy). Note: this is a character based model. 

We chose to use a recurrent neural network because RNNs can “make use of sequential information” [1] and have been used to achieve great success in various natural language processing tasks. In particular, recurrent neural networks are powerful for this task because they are able to predict the likelihood of the next letter given the previous letter, meaning the output of the trained model would be the sequence of predicted letters.

https://github.com/spiglerg/RNN_Text_Generation_Tensorflow

The following model is a word level model created by hunkim. We wanted to compare the performance of character based and word based models. The model took 3-4 days to train on a training set of 5000 songs (filesize approx. 3MB). Due to the time constraints, we were unable to expand more upon this in our final report. Ideally, we would have liked to compare the performance of word level RNNs to char level RNNs and explore why char level RNNs are more favorable. 
https://github.com/hunkim/word-rnn-tensorflow

### Text Comparison Tool

This is a tool that can be used to identify common words between the input text and the output file (see common-words.py). What is more useful however, is the ngram-finder.py, which finds sequences of words (up to length n) that the texts have in common.
https://github.com/alimony/text-comparison-tools

## Results
### Description of contents for the following result folders: (1) markov_model, (2) rnn_char_level, (3) rnn_word_level
Lyrics generated by each model were trained on the following datasets, which can be found either in this repository under training_data or in our Google Drive Milestone 8 folder. The number in parenthenses refers to the corresponding numbers above:
- dolly_parton.txt (1,2)
- sample_1000.txt (1,2)
- sample_2500.txt (1,2)
- sample_5000.txt (1,2,3)
- year_2006.txt (2, 10 songs for testing purposes)

For both RNNs, the loss was also recorded and the data needed to reconstruct the models were saved. 

## Credits
[1] http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
